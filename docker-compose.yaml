services:
  llama-cpp-cpu:
    profiles:
      - cpu
    build:
      context: ./apps/llama-cpp
      dockerfile: ./docker/cpu.Dockerfile
    restart: unless-stopped
    ports:
      - 8000:8000
    volumes:
      - './apps/llama-cpp/huggingface:/app/huggingface'

  llama-cpp-cuda:
    profiles:
      - cuda
    build:
      context: ./apps/llama-cpp
      dockerfile: ./docker/cuda.Dockerfile
    restart: unless-stopped
    ports:
      - 8000:8000
    volumes:
      - './apps/llama-cpp/huggingface:/app/huggingface'
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  pocketbase:
    build: ./apps/pocketbase
    restart: unless-stopped
    ports:
      - 8080:8080
    volumes:
      - './apps/pocketbase/pb_data:/pb/pb_data'
      - './apps/pocketbase/pb_migrations:/pb/pb_migrations'

  web:
    build:
      context: ./apps/web
      args:
        - PUBLIC_POCKETBASE_BASE_URL=http://localhost:8080
    restart: unless-stopped
    environment:
      - POCKETBASE_BASE_URL=http://pocketbase:8080
    ports:
      - 3000:3000

  ngrok-web:
    profiles:
      - ngrok-web
    image: ngrok/ngrok:latest
    restart: unless-stopped
    command:
      - 'start'
      - 'web'
      - '--config'
      - '/etc/ngrok.yml'
    volumes:
      - ./ngrok.yml:/etc/ngrok.yml
    ports:
      - 4040:4040

  ngrok-llama-cpp:
    profiles:
      - ngrok-llama-cpp
    image: ngrok/ngrok:latest
    restart: unless-stopped
    command:
      - 'start'
      - 'llama-cpp'
      - '--config'
      - '/etc/ngrok.yml'
    volumes:
      - ./ngrok.yml:/etc/ngrok.yml
    ports:
      - 4040:4040
